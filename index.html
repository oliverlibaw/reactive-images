<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Eye Detection</title>
    <style>
        /* Add custom CSS styling here */
    </style>
    <!-- Ignore source map error -->
    <meta name="sourceMapURL" content="">
    <!-- Import TensorFlow.js library -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.7.4"></script>
    <!-- Import face-landmarks-detection library -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/face-landmarks-detection"></script>
</head>
<body>
    <!-- Display image of person with eyes open or closed -->
    <img id="person" src="eyes_open.jpg" alt="Titian Painting" />

    <script>
        let model;

        // Access the user's webcam
        async function getUserMediaStream() {
            try {
                const constraints = {
                    video: {
                        facingMode: 'user'
                    },
                    audio: false
                };

                return await new Promise((resolve, reject) => {
                    navigator.getUserMedia = navigator.getUserMedia ||
                                              navigator.webkitGetUserMedia ||
                                              navigator.mozGetUserMedia ||
                                              navigator.msGetUserMedia;
                    if (navigator.getUserMedia) {
                        navigator.getUserMedia(constraints, resolve, reject);
                    } else {
                        reject(new Error('getUserMedia not supported in this browser.'));
                    }
                });
            } catch (err) {
                console.error('Error accessing the webcam:', err);
            }
        }

        // Load the face-landmarks-detection models
        async function loadModels() {
          model = await faceLandmarksDetection.load(
            faceLandmarksDetection.SupportedPackages.mediapipeFacemesh
          );
        }

        // Detect eyes in the webcam feed using the face-landmarks-detection library
        async function detectEyes(video) {
          const options = { maxFaces: 1 };
          const predictions = await model.estimateFaces({ input: video, ...options });

          if (predictions && predictions.length > 0) {
            const leftEye = predictions[0].annotations.leftEyeIris[0];
            const rightEye = predictions[0].annotations.rightEyeIris[0];

            if (leftEye && rightEye) {
              return true;
            }
          }
          return false;
        }

        // Main function to run the eye detection
        async function runEyeDetection() {
            // Access user's webcam
            const stream = await getUserMediaStream();
            const video = document.createElement('video');
            video.setAttribute('playsinline', '');
            video.srcObject = stream;
            video.onloadedmetadata = () => {
                video.play();
            };

            // Load the face-landmarks-detection models
            await loadModels();

            // Run the eye detection and update the image
            const personImage = document.getElementById('person');
            setInterval(async () => {
                const eyesDetected = await detectEyes(video);

                if (eyesDetected) {
                    personImage.src = 'eyes_open.jpg';
                } else {
                    personImage.src = 'eyes_closed.jpg';
                }
            }, 1000);
        }

        // Run the eye detection on page load
        window.addEventListener('load', runEyeDetection);
    </script>
</body>
</html>
