<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Eye Detection</title>
    <style>
        /* Add custom CSS styling here */
    </style>
    <!-- Ignore source map error -->
    <meta name="sourceMapURL" content="">
    <!-- Import TensorFlow.js library -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.7.4"></script>
    <!-- Import facedetect.js library -->
    <script src="https://unpkg.com/@vladmandic/facedetect@latest/app.esm.js"></script>
</head>
<body>
    <!-- Display image of person with eyes open or closed -->
    <img id="person" src="" alt="Person image" />

    <script>
        // Access the user's webcam
        async function getUserMediaStream() {
            try {
                const constraints = {
                    video: true,
                    audio: false
                };
                return await navigator.mediaDevices.getUserMedia(constraints);
            } catch (err) {
                console.error('Error accessing the webcam:', err);
            }
        }

        // Load the facedetect.js models
        async function loadModels() {
            await facedetect.ready();
        }

        // Detect eyes in the webcam feed using the facedetect.js library
        async function detectEyes(video) {
            const options = { maxResults: 1, fast: true };
            const result = await facedetect.process(video, options);

            if (result && result.length > 0 && result[0].landmarks) {
                const eyeLeft = result[0].landmarks.get("leftEye");
                const eyeRight = result[0].landmarks.get("rightEye");

                if (eyeLeft && eyeRight) {
                    return true;
                }
            }
            return false;
        }

        // Main function to run the eye detection
        async function runEyeDetection() {
            // Access user's webcam
            const stream = await getUserMediaStream();
            const video = document.createElement('video');
            video.srcObject = stream;
            video.onloadedmetadata = () => {
                video.play();
            };

            // Load the face-api.js models
            await loadModels();

            // Run the eye detection and update the image
            const personImage = document.getElementById('person');
            setInterval(async () => {
                const eyesDetected = await detectEyes(video);

                if (eyesDetected) {
                    personImage.src = 'eyes_open.jpg';
                } else {
                    personImage.src = 'eyes_closed.jpg';
                }
            }, 1000);
        }

        // Run the eye detection on page load
        window.addEventListener('load', runEyeDetection);
    </script>
</body>
</html>
